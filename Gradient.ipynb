{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[0.5,2.5]\n",
    "Y=[0.2,0.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w,b,x):\n",
    "    return 1/(1+(np.exp(-(w*x-b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradW(w,b,x,y):\n",
    "    fx = sigmoid(w,b,x)\n",
    "    return (fx - y) * (fx) * (1 - fx) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradB(w,b,x,y):\n",
    "    fx = sigmoid(w,b,x)\n",
    "    return (fx - y) * (fx) * (1 - fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFuction(w,b):\n",
    "    error = 0\n",
    "    for (dx,dy) in zip(X,Y):\n",
    "        fx = sigmoid(w,b,dx)\n",
    "        error += 0.5 * (fx - dy)**2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    w,b,learning_rate,epoch = 1,2,0.01,0\n",
    "    for i in range(epoch):\n",
    "        dw,db = 0,0\n",
    "        for (x,y) in zip(X,Y):\n",
    "            dw += gradW(w,b,x,y)\n",
    "            db += gradB(w,b,x,y)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        print(lossFuction(w,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main() # add epoches before running "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Weight</th>\n",
    "        <th>Bais</th>\n",
    "        <th>Learning rate</th>\n",
    "        <th>epoch</th>\n",
    "        <th>Loss</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>1</td>\n",
    "        <td>1000</td>\n",
    "        <td>0.31866548026885827</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.1</td>\n",
    "         <td>1000</td>\n",
    "        <td>0.30593543739290247</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.01</td>\n",
    "         <td>1000</td>\n",
    "        <td>0.2427620803813011</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.001</td>\n",
    "         <td>1000</td>\n",
    "        <td>0.505558163251419</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>1</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.01</td>\n",
    "         <td>1000</td>\n",
    "        <td>0.2808141439267926</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0.01</td>\n",
    "         <td>1000</td>\n",
    "        <td>0.16871174594893576</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>2</td>\n",
    "        <td>0.01</td>\n",
    "         <td>1000</td>\n",
    "        <td>0.0017835721698263975</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "        <td>1</td>\n",
    "        <td>2</td>\n",
    "        <td>0.01</td>\n",
    "         <td>2000</td>\n",
    "        <td>0.00043200359300054117</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>2</td>\n",
    "        <td>0.01</td>\n",
    "         <td>3000</td>\n",
    "        <td>0.0001908892526135591</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<h3>Conclusion</h3>\n",
    "<h5>Learning Rate makes a difference till a certain point but if we decreased it too much algo. will not converge.</h5>\n",
    "<h5>Weight and bais values make a great difference as we can see loss decreasing as we change the values for 1000 epochs.</h5>\n",
    "<h5>The number of epochs reduces loss but the loss is under 0.000 after 2000 so doing more take it down but not required as it will not make a significant change.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
