{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[0.5,2.5]\n",
    "Y=[0.2,0.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w,b,x):\n",
    "    return 1/(1+(np.exp(-(w*x-b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradW(w,b,x,y):\n",
    "    fx = sigmoid(w,b,x)\n",
    "    return (fx - y) * (fx) * (1 - fx) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradB(w,b,x,y):\n",
    "    fx = sigmoid(w,b,x)\n",
    "    return (fx - y) * (fx) * (1 - fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFuction(w,b):\n",
    "    error = 0\n",
    "    for (dx,dy) in zip(X,Y):\n",
    "        fx = sigmoid(w,b,dx)\n",
    "        error += 0.5 * (fx - dy)**2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanila_grad(w,b,learning_rate,epochs):\n",
    "    seconds = time.time()\n",
    "    for i in range(epochs):\n",
    "        dw,db = 0,0\n",
    "        for (x,y) in zip(X,Y):\n",
    "            dw += gradW(w,b,x,y)\n",
    "            db += gradB(w,b,x,y)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        print(lossFuction(w,b))\n",
    "    print(\"Seconds since epoch =\", time.time() - seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moumentum_grad(w,b,learning_rate,epochs):\n",
    "    seconds = time.time()\n",
    "    gama = 0.1\n",
    "    w_pre,b_pre = 0,0\n",
    "    for i in range(epochs):\n",
    "        dw,db = 0,0\n",
    "        for (x,y) in zip(X,Y):\n",
    "            dw += gradW(w,b,x,y)\n",
    "            db += gradB(w,b,x,y)\n",
    "        w = w - ((gama * w_pre) +(learning_rate * dw))\n",
    "        b = b - ((gama * b_pre) +(learning_rate * db))\n",
    "        w_pre = w\n",
    "        b_pre = b\n",
    "        print(lossFuction(w,b))\n",
    "    print(\"Seconds since epoch =\", time.time() - seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5046873188453432\n",
      "0.5048992456379796\n",
      "0.5050876025573474\n",
      "0.5052512236473746\n",
      "0.505388878376643\n",
      "0.5054992670295545\n",
      "0.5055810157331817\n",
      "0.5056326710907652\n",
      "0.5056526943908577\n",
      "0.5056394553591271\n",
      "0.5055912254178838\n",
      "0.5055061704165345\n",
      "0.505382342794479\n",
      "0.5052176731365383\n",
      "0.5050099610799818\n",
      "0.5047568655317709\n",
      "0.5044558941549739\n",
      "0.5041043920846991\n",
      "0.5036995298367204\n",
      "0.5032382903766361\n",
      "0.5027174553244939\n",
      "0.5021335902800017\n",
      "0.5014830292676171\n",
      "0.5007618583199975\n",
      "0.49996589824384485\n",
      "0.4990906866456812\n",
      "0.4981314593384962\n",
      "0.49708313130589565\n",
      "0.4959402774711194\n",
      "0.4946971136074841\n",
      "0.4933474778382625\n",
      "0.4918848133122474\n",
      "0.4903021528112296\n",
      "0.48859210625279914\n",
      "0.4867468523019164\n",
      "0.48475813560307657\n",
      "0.4826172714963104\n",
      "0.480315160487618\n",
      "0.4778423152074459\n",
      "0.47518890310389655\n",
      "0.4723448086669465\n",
      "0.46929971954071203\n",
      "0.46604324141121367\n",
      "0.4625650469938723\n",
      "0.45885506469731924\n",
      "0.45490371248411665\n",
      "0.45070218192404216\n",
      "0.44624277624484754\n",
      "0.441519304104135\n",
      "0.4365275276018815\n",
      "0.4312656585238414\n",
      "0.4257348908378943\n",
      "0.419939950114646\n",
      "0.41388963213450963\n",
      "0.4075972941674495\n",
      "0.4010812543965727\n",
      "0.3943650492635871\n",
      "0.38747749700807455\n",
      "0.3804525202303838\n",
      "0.37332869235829896\n",
      "0.3661484928587549\n",
      "0.3589572827983458\n",
      "0.35180204302042634\n",
      "0.3447299472791572\n",
      "0.33778686682201564\n",
      "0.3310159161886549\n",
      "0.3244561491631055\n",
      "0.31814149838386996\n",
      "0.3121000246126817\n",
      "0.3063535070454226\n",
      "0.3009173704118532\n",
      "0.2958009136528694\n",
      "0.2910077827302866\n",
      "0.28653661837787797\n",
      "0.28238180789995837\n",
      "0.2785342764168509\n",
      "0.27498226441726603\n",
      "0.2717120522611803\n",
      "0.26870860604596497\n",
      "0.26595613141362245\n",
      "0.2634385316004308\n",
      "0.26113977308597525\n",
      "0.25904416677557135\n",
      "0.2571365751640191\n",
      "0.2554025568739689\n",
      "0.25382845981515556\n",
      "0.2524014733767751\n",
      "0.25110964885967935\n",
      "0.2499418960045799\n",
      "0.2488879621276852\n",
      "0.24793839912691795\n",
      "0.2470845225172321\n",
      "0.2463183657099781\n",
      "0.24563263196727556\n",
      "0.24502064582570507\n",
      "0.2444763052766987\n",
      "0.24399403559453603\n",
      "0.24356874539794593\n",
      "0.24319578530084485\n",
      "0.2428709093366296\n",
      "Seconds since epoch = 0.00393986701965332\n",
      "-----------------------------\n",
      "0.5046873188453432\n",
      "0.48090575326959506\n",
      "0.4556926799627107\n",
      "0.42920264271829794\n",
      "0.40172974638958914\n",
      "0.3737046884803442\n",
      "0.34567098069893415\n",
      "0.3182368244303828\n",
      "0.29200671846316006\n",
      "0.26750690145032485\n",
      "0.245124585733259\n",
      "0.22507733646102512\n",
      "0.20741718485822527\n",
      "0.19206147677475088\n",
      "0.17883603813559965\n",
      "0.16751742466249384\n",
      "0.157866441113805\n",
      "0.14965065444762693\n",
      "0.14265714023355403\n",
      "0.13669806796427464\n",
      "0.13161172809317215\n",
      "0.12726102572033138\n",
      "0.1235308017848551\n",
      "0.1203248001744468\n",
      "0.11756272237394727\n",
      "0.11517757570421115\n",
      "0.11311338596385476\n",
      "0.11132327420142613\n",
      "0.10976786429571096\n",
      "0.10841397607139777\n",
      "0.10723355759503933\n",
      "0.10620281423692862\n",
      "0.10530149788318313\n",
      "0.10451232573266254\n",
      "0.10382050367722732\n",
      "0.1032133340654674\n",
      "0.10267989164574914\n",
      "0.10221075473675038\n",
      "0.1017977812848822\n",
      "0.10143392154720515\n",
      "0.10111306078638613\n",
      "0.10082988666772003\n",
      "0.10057977707912605\n",
      "0.10035870491138085\n",
      "0.10016315698386072\n",
      "0.0999900648170931\n",
      "0.09983674536592152\n",
      "0.0997008501582574\n",
      "0.09958032155148805\n",
      "0.09947335503507101\n",
      "0.0993783666841311\n",
      "0.09929396501315682\n",
      "0.09921892659754614\n",
      "0.09915217492879641\n",
      "0.099092762050518\n",
      "0.09903985259029377\n",
      "0.09899270985919992\n",
      "0.09895068373852106\n",
      "0.09891320011343194\n",
      "0.09887975164745971\n",
      "0.09884988972042735\n",
      "0.09882321737715849\n",
      "0.09879938315518874\n",
      "0.09877807567765817\n",
      "0.09875901891291369\n",
      "0.09874196801553592\n",
      "0.0987267056748411\n",
      "0.09871303890666788\n",
      "0.09870079623268249\n",
      "0.09868982519870172\n",
      "0.09867999018982303\n",
      "0.09867117050559018\n",
      "0.0986632586631376\n",
      "0.09865615890034604\n",
      "0.09864978585459042\n",
      "0.09864406339574579\n",
      "0.098638923594797\n",
      "0.09863430581173274\n",
      "0.0986301558884349\n",
      "0.09862642543404535\n",
      "0.09862307119183696\n",
      "0.09862005447796157\n",
      "0.09861734068362596\n",
      "0.09861489883327346\n",
      "0.09861270119224812\n",
      "0.09861072291820605\n",
      "0.09860894175122373\n",
      "0.09860733773816051\n",
      "0.09860589298735706\n",
      "0.09860459145021802\n",
      "0.09860341872663188\n",
      "0.0986023618915402\n",
      "0.09860140934028078\n",
      "0.09860055065060613\n",
      "0.098599776459521\n",
      "0.09859907835329713\n",
      "0.0985984487692105\n",
      "0.09859788090771412\n",
      "0.09859736865390323\n",
      "0.09859690650726119\n",
      "Seconds since epoch = 0.010802745819091797\n"
     ]
    }
   ],
   "source": [
    "vanila_grad(-2,-2,0.1,100)\n",
    "print(\"-----------------------------\")\n",
    "moumentum_grad(-2,-2,0.1,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Weight</th>\n",
    "        <th>Bais</th>\n",
    "        <th>Learning rate</th>\n",
    "        <th>epoch</th>\n",
    "        <th>Vanila-Gradient-Time</th>\n",
    "        <th>Momentum-Gradient-Time</th>\n",
    "        <th>Vanila-Gradient-Loss</th>\n",
    "        <th>Momentum-Gradient-Loss</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.1</td>\n",
    "        <td>10</td>\n",
    "        <td>0.0005846023559570312</td>\n",
    "        <td>0.00041937828063964844</td>\n",
    "        <td>0.5056394553591271</td>\n",
    "        <td>0.26750690145032485</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.1</td>\n",
    "        <td>50</td>\n",
    "        <td>0.002499103546142578</td>\n",
    "        <td>0.0017952919006347656</td>\n",
    "        <td>0.4365275276018815</td>\n",
    "        <td>0.09947335503507101</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.1</td>\n",
    "        <td>75</td>\n",
    "        <td>0.0009799003601074219</td>\n",
    "        <td>0.0008487701416015625</td>\n",
    "        <td>0.28238180789995837</td>\n",
    "        <td>0.09864978585459042</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>-2</td>\n",
    "        <td>-2</td>\n",
    "        <td>0.1</td>\n",
    "        <td>100</td>\n",
    "        <td>0.00393986701965332</td>\n",
    "        <td>0.010802745819091797</td>\n",
    "        <td>0.2428709093366296</td>\n",
    "        <td>0.09859690650726119</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
